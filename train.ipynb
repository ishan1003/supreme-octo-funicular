{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb96a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn_train_infer.py  (heavier/deeper)\n",
    "import os, sys, json, pickle, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- PyG ----\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from dataset_loader import load_sample, build_node_features, get_available_ids, split_ids\n",
    "\n",
    "# ========= config =========\n",
    "ROOT = Path(\"./dataset/dataset_generation/data\")\n",
    "MAX_ID_SCAN   = 20000\n",
    "NUM_CLASSES   = 25  # len(feat_names) below\n",
    "EPOCHS        = 200\n",
    "HIDDEN        = 300     # wider\n",
    "LAYERS        = 6       # deeper\n",
    "DROPOUT       = 0.30\n",
    "BATCH_SIZE    = 64\n",
    "LR            = 3e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "LABEL_SMOOTH  = 0.05\n",
    "CLIP_NORM     = 1.0\n",
    "PATIENCE      = 10\n",
    "SEED          = 13\n",
    "USE_COMPILE   = True     # will fall back to eager on Windows/no Triton\n",
    "\n",
    "feat_names = [\n",
    "    'chamfer', 'through_hole', 'triangular_passage', 'rectangular_passage', '6sides_passage',\n",
    "    'triangular_through_slot', 'rectangular_through_slot', 'circular_through_slot',\n",
    "    'rectangular_through_step', '2sides_through_step', 'slanted_through_step', 'Oring', 'blind_hole',\n",
    "    'triangular_pocket', 'rectangular_pocket', '6sides_pocket', 'circular_end_pocket',\n",
    "    'rectangular_blind_slot', 'v_circular_end_blind_slot', 'h_circular_end_blind_slot',\n",
    "    'triangular_blind_step', 'circular_blind_step', 'rectangular_blind_step', 'round', 'stock'\n",
    "]\n",
    "\n",
    "# ---- speed knobs for A40 ----\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ========= PyG dataset =========\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root: Path, ids, cache_mode=\"lazy\", strict=True):\n",
    "        from tqdm import tqdm\n",
    "        self.root = Path(root)\n",
    "        self.ids = []\n",
    "        self._cache = {} if cache_mode in (\"lazy\", \"all\") else None\n",
    "        self._lazy = (cache_mode == \"lazy\")\n",
    "\n",
    "        for i in tqdm(ids, desc=\"Indexing graphs\"):\n",
    "            try:\n",
    "                d = load_sample(self.root, i)\n",
    "                if strict and (d[\"y\"].numel() != d[\"x\"].shape[0]):\n",
    "                    continue\n",
    "                self.ids.append(i)\n",
    "                if cache_mode == \"all\" or (self._cache is not None and not self._lazy):\n",
    "                    self._cache[i] = self._to_data(d)\n",
    "            except Exception:\n",
    "                continue\n",
    "        if not self.ids:\n",
    "            raise RuntimeError(\"No usable graphs after filtering.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_data(d):\n",
    "        return Data(x=d[\"x\"].float(),\n",
    "                    edge_index=d[\"edge_index\"].long(),\n",
    "                    y=d[\"y\"].long())\n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.ids[idx]\n",
    "        if self._cache is not None and pid in self._cache:\n",
    "            return self._cache[pid]\n",
    "        d = load_sample(self.root, pid)\n",
    "        g = self._to_data(d)\n",
    "        if self._cache is not None and self._lazy:\n",
    "            self._cache[pid] = g\n",
    "        return g\n",
    "\n",
    "# ========= model (deeper/heavier) =========\n",
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_ch, out_ch)\n",
    "        self.bn   = nn.BatchNorm1d(out_ch)\n",
    "        self.p    = p_drop\n",
    "        self.res  = (in_ch == out_ch)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        out = self.conv(x, edge_index)\n",
    "        out = self.bn(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = F.dropout(out, p=self.p, training=self.training)\n",
    "        if self.res:\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "class DeepGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=256, layers=4, out_dim=NUM_CLASSES, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.in_lin = nn.Linear(in_dim, hidden)\n",
    "        blocks = []\n",
    "        for _ in range(layers):\n",
    "            blocks.append(GCNBlock(hidden, hidden, p_drop=dropout))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        # 2-layer MLP head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.in_lin(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, edge_index)\n",
    "        return self.head(x)\n",
    "\n",
    "# ========= training helpers =========\n",
    "def class_weights_from_dataset(ds):\n",
    "    counts = torch.zeros(NUM_CLASSES, dtype=torch.long)\n",
    "    for g in ds:\n",
    "        counts += torch.bincount(g.y, minlength=NUM_CLASSES)\n",
    "    w = 1.0 / (counts.float() + 1e-6)\n",
    "    w *= (NUM_CLASSES / w.sum())\n",
    "    return w\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_labels = 0.0, 0, 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\"), dtype=torch.float16):\n",
    "            logits = model(batch.x, batch.edge_index)\n",
    "            loss = F.cross_entropy(logits, batch.y, label_smoothing=LABEL_SMOOTH)\n",
    "        total_loss += float(loss) * batch.y.numel()\n",
    "        total_correct += (logits.argmax(dim=1) == batch.y).sum().item()\n",
    "        total_labels += batch.y.numel()\n",
    "    return total_loss / max(1, total_labels), total_correct / max(1, total_labels)\n",
    "\n",
    "def train_one_epoch(model, device, loader, opt, scaler, weight, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_labels = 0.0, 0, 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\"), dtype=torch.float16):\n",
    "            logits = model(batch.x, batch.edge_index)\n",
    "            w = weight.to(device=device, dtype=logits.dtype) if weight is not None else None\n",
    "            loss = F.cross_entropy(logits, batch.y, weight=w, label_smoothing=LABEL_SMOOTH)\n",
    "        scaler.scale(loss).backward()\n",
    "        # clip on unscaled grads\n",
    "        scaler.unscale_(opt)\n",
    "        if CLIP_NORM is not None and CLIP_NORM > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        total_loss += float(loss) * batch.y.numel()\n",
    "        total_correct += (logits.argmax(dim=1) == batch.y).sum().item()\n",
    "        total_labels += batch.y.numel()\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    return total_loss / max(1, total_labels), total_correct / max(1, total_labels)\n",
    "\n",
    "# ========= inference utility =========\n",
    "@torch.no_grad()\n",
    "def predict_part(model, device, root: Path, part_id: int):\n",
    "    d = load_sample(root, part_id)\n",
    "    x = d[\"x\"].to(device)\n",
    "    ei = d[\"edge_index\"].to(device)\n",
    "    logits = model(x, ei)\n",
    "    yhat = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "    return [feat_names[i] for i in yhat]\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# ========= run =========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "avail = get_available_ids(ROOT, max_id=MAX_ID_SCAN)\n",
    "print(f\"usable parts on disk: {len(avail)}\")\n",
    "if len(avail) < 50:\n",
    "    raise RuntimeError(\"Too few usable samples; generate more.\")\n",
    "\n",
    "ds_tr_ids, ds_va_ids, ds_te_ids = split_ids(avail, seed=SEED)\n",
    "print(f\"split -> train {len(ds_tr_ids)}, val {len(ds_va_ids)}, test {len(ds_te_ids)}\")\n",
    "\n",
    "# build datasets (lazy cache), avoid Windows/Jupyter multiprocessing stalls\n",
    "IS_WINDOWS = (sys.platform == \"win32\")\n",
    "IN_NOTEBOOK = (\"ipykernel\" in sys.modules)\n",
    "NUM_WORKERS = 0 if (IS_WINDOWS or IN_NOTEBOOK) else 4\n",
    "pin = torch.cuda.is_available() and NUM_WORKERS > 0\n",
    "\n",
    "ds_tr = GraphDataset(ROOT, ds_tr_ids, cache_mode=\"lazy\", strict=True)\n",
    "ds_va = GraphDataset(ROOT, ds_va_ids, cache_mode=\"lazy\", strict=True)\n",
    "ds_te = GraphDataset(ROOT, ds_te_ids, cache_mode=\"none\", strict=True)\n",
    "\n",
    "tr_loader = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "va_loader = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "te_loader = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "\n",
    "# infer feature dimension from one item\n",
    "g0 = ds_tr[0]\n",
    "in_dim = g0.x.shape[1]\n",
    "print(f\"in_dim: {in_dim}\")\n",
    "\n",
    "model = DeepGCN(in_dim, hidden=HIDDEN, layers=LAYERS, out_dim=NUM_CLASSES, dropout=DROPOUT).to(device)\n",
    "print(f\"Model params: {count_params(model)/1e6:.2f}M\")\n",
    "\n",
    "# optional compile (Windows-safe)\n",
    "if USE_COMPILE:\n",
    "    import importlib.util, torch._dynamo as dynamo\n",
    "    IS_WINDOWS = (sys.platform == \"win32\")\n",
    "    HAS_TRITON = importlib.util.find_spec(\"triton\") is not None\n",
    "    try:\n",
    "        if (not IS_WINDOWS) and HAS_TRITON:\n",
    "            print(\"torch.compile: backend='inductor'\")\n",
    "            model = torch.compile(model)\n",
    "        else:\n",
    "            print(\"torch.compile: backend='eager' (Windows/no Triton)\")\n",
    "            dynamo.config.suppress_errors = True\n",
    "            model = torch.compile(model, backend=\"eager\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = GradScaler(enabled=(device.type==\"cuda\"))\n",
    "W = class_weights_from_dataset(ds_tr)\n",
    "\n",
    "# cosine schedule (epoch-wise)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "\n",
    "# train\n",
    "hist = {\"tr_loss\": [], \"va_loss\": [], \"tr_acc\": [], \"va_acc\": []}\n",
    "best_val, best_state, bad = float(\"inf\"), None, 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, device, tr_loader, opt, scaler, W, scheduler)\n",
    "    va_loss, va_acc = evaluate(model, device, va_loader)\n",
    "\n",
    "    hist[\"tr_loss\"].append(tr_loss); hist[\"va_loss\"].append(va_loss)\n",
    "    hist[\"tr_acc\"].append(tr_acc);   hist[\"va_acc\"].append(va_acc)\n",
    "\n",
    "    print(f\"ep{epoch:03d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f} | lr {opt.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if va_loss < best_val - 1e-4:\n",
    "        best_val = va_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# save checkpoint\n",
    "ckpt_dir = Path(\"./checkpoints\"); ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_path = ckpt_dir / \"gcn_facecls.pt\"\n",
    "torch.save({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"arch\": \"DeepGCN\",\n",
    "    \"in_dim\": in_dim,\n",
    "    \"hidden\": HIDDEN,\n",
    "    \"layers\": LAYERS,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "    \"feat_names\": feat_names,\n",
    "    \"train_ids\": ds_tr.ids, \"val_ids\": ds_va.ids, \"test_ids\": ds_te.ids,\n",
    "}, ckpt_path)\n",
    "print(f\"Saved checkpoint -> {ckpt_path}\")\n",
    "\n",
    "# test\n",
    "te_loss, te_acc = evaluate(model, device, te_loader)\n",
    "print(f\"TEST  loss/acc: {te_loss:.4f}/{te_acc:.3f}\")\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(hist[\"tr_loss\"], label=\"train\")\n",
    "plt.plot(hist[\"va_loss\"], label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\")\n",
    "plt.title(\"DeepGCN per-face classification\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick demo inference\n",
    "demo_id = random.choice(ds_te.ids)\n",
    "pred_names = predict_part(model, device, ROOT, demo_id)\n",
    "print(f\"\\nDemo predictions for part {demo_id}:\")\n",
    "print(pred_names[:min(20, len(pred_names))], f\"... (total faces={len(pred_names)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- quick demo inference + flat GT comparison (no functions) ---\n",
    "demo_id = random.choice(ds_te.ids)\n",
    "print(f\"\\n--- Demo part {demo_id} ---\")\n",
    "\n",
    "# load sample + move to device\n",
    "d  = load_sample(ROOT, demo_id)          # also reads ground-truth JSON\n",
    "x  = d[\"x\"].to(device)\n",
    "ei = d[\"edge_index\"].to(device)\n",
    "y  = d[\"y\"].cpu().numpy()                # ground-truth indices\n",
    "\n",
    "# forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\"), dtype=torch.float16):\n",
    "        logits = model(x, ei)            # [num_faces, NUM_CLASSES]\n",
    "\n",
    "# predictions and names\n",
    "pred_idx   = logits.argmax(dim=1).cpu().numpy()\n",
    "pred_names = [feat_names[i] for i in pred_idx]\n",
    "gt_names   = [feat_names[i] for i in y]\n",
    "\n",
    "# accuracy\n",
    "correct = int((pred_idx == y).sum())\n",
    "total   = int(y.size)\n",
    "acc     = correct / max(1, total)\n",
    "print(f\"faces={total}  acc={acc:.3f}  ({correct}/{total})\")\n",
    "\n",
    "# show a few mismatches with top-3 alternatives\n",
    "wrong = np.flatnonzero(pred_idx != y)\n",
    "max_show = 20\n",
    "if wrong.size:\n",
    "    probs = logits.softmax(dim=1).cpu().numpy()\n",
    "    k = min(max_show, wrong.size)\n",
    "    print(f\"First {k} mismatches (face_idx: pred (p) -> gt | top3):\")\n",
    "    for i in wrong[:k]:\n",
    "        top3 = probs[i].argsort()[-3:][::-1]\n",
    "        top3_str = \", \".join(f\"{feat_names[t]}({probs[i][t]:.2f})\" for t in top3)\n",
    "        print(f\"  {i:4d}: {feat_names[pred_idx[i]]} ({probs[i][pred_idx[i]]:.2f})\"\n",
    "              f\" -> {feat_names[y[i]]} | {top3_str}\")\n",
    "else:\n",
    "    print(\"No mismatches on this part 🎉\")\n",
    "\n",
    "# optional: per-part classification report (requires scikit-learn)\n",
    "try:\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"\\nClassification report (this part only):\")\n",
    "    print(classification_report(\n",
    "        y, pred_idx,\n",
    "        labels=list(range(NUM_CLASSES)),\n",
    "        target_names=feat_names,\n",
    "        digits=3,\n",
    "        zero_division=0\n",
    "    ))\n",
    "except Exception as e:\n",
    "    print(f\"[sklearn report skipped] {e}\")\n",
    "\n",
    "# optional: peek a few names\n",
    "print(\"\\nPred (first 20):\", pred_names[:20])\n",
    "print(\"GT   (first 20):\", gt_names[:20])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
