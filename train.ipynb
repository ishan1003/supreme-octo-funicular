{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch_geometric, torch_scatter, torch_sparse\n",
    "print(\"CUDA avail:\", torch.cuda.is_available(), \"torch CUDA:\", torch.version.cuda)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb96a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn_train_infer.py\n",
    "import os, sys, json, pickle, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- PyG ----\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# ========= config =========\n",
    "ROOT = Path(\"./dataset/dataset_generation/data\")\n",
    "MAX_ID_SCAN = 10000\n",
    "NUM_CLASSES = 25  # len(feat_names) below\n",
    "EPOCHS = 60\n",
    "HIDDEN = 128\n",
    "BATCH_SIZE = 64\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 8\n",
    "SEED = 13\n",
    "\n",
    "feat_names = ['chamfer', 'through_hole', 'triangular_passage', 'rectangular_passage', '6sides_passage',\n",
    "              'triangular_through_slot', 'rectangular_through_slot', 'circular_through_slot',\n",
    "              'rectangular_through_step', '2sides_through_step', 'slanted_through_step', 'Oring', 'blind_hole',\n",
    "              'triangular_pocket', 'rectangular_pocket', '6sides_pocket', 'circular_end_pocket',\n",
    "              'rectangular_blind_slot', 'v_circular_end_blind_slot', 'h_circular_end_blind_slot',\n",
    "              'triangular_blind_step', 'circular_blind_step', 'rectangular_blind_step', 'round', 'stock']\n",
    "\n",
    "# ---- speed knobs for A40 ----\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ========= data utils (from your dataset_loader, simplified) =========\n",
    "def get_available_ids(root: Path, max_id=1000):\n",
    "    root = Path(root)\n",
    "    have = []\n",
    "    for i in range(max_id):\n",
    "        if (root / f\"{i}.pkl\").exists() and (root / \"labels\" / f\"{i}.json\").exists():\n",
    "            have.append(i)\n",
    "    return have\n",
    "\n",
    "def split_ids(ids, seed=13, frac=(0.8, 0.1, 0.1)):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ids = np.array(ids, dtype=int)\n",
    "    rng.shuffle(ids)\n",
    "    n = len(ids)\n",
    "    n_tr = int(frac[0]*n)\n",
    "    n_va = int(frac[1]*n)\n",
    "    train = ids[:n_tr].tolist()\n",
    "    val   = ids[n_tr:n_tr+n_va].tolist()\n",
    "    test  = ids[n_tr+n_va:].tolist()\n",
    "    return train, val, test\n",
    "\n",
    "def _one_hot(idx, n):\n",
    "    v = np.zeros(n, dtype=np.float32); v[int(idx)] = 1.0; return v\n",
    "\n",
    "def build_node_features(face_feats: dict, use_type_onehot=False):\n",
    "    \"\"\"\n",
    "    Returns float32 [num_nodes, D] matrix from face_features dict.\n",
    "    Included: area, adj(deg), loops, centroid(3), convexity(one-hot 3), type(scalar) => D=10\n",
    "    \"\"\"\n",
    "    n = len(face_feats['area'])\n",
    "    area      = np.asarray(face_feats['area'], dtype=np.float32).reshape(n,1)\n",
    "    deg       = np.asarray(face_feats['adj'], dtype=np.float32).reshape(n,1)\n",
    "    loops     = np.asarray(face_feats['loops'], dtype=np.float32).reshape(n,1)\n",
    "    centroid  = np.asarray(face_feats['centroid'], dtype=np.float32)  # [n,3]\n",
    "    conv      = np.asarray(face_feats['convexity'], dtype=np.int64)\n",
    "    conv_oh   = np.stack([_one_hot(c, 3) for c in conv], axis=0)      # [n,3]\n",
    "\n",
    "    parts = [area, deg, loops, centroid, conv_oh]\n",
    "\n",
    "    if use_type_onehot:\n",
    "        stype = np.asarray(face_feats['type'], dtype=np.int64)\n",
    "        S = int(stype.max()) + 1\n",
    "        stype_oh = np.stack([_one_hot(t, S) for t in stype], axis=0)\n",
    "        parts.append(stype_oh)\n",
    "    else:\n",
    "        parts.append(np.asarray(face_feats['type'], dtype=np.float32).reshape(n,1))\n",
    "\n",
    "    x = np.concatenate(parts, axis=1).astype(np.float32)\n",
    "    # light normalization\n",
    "    x[:,0] = (x[:,0] - x[:,0].mean()) / (x[:,0].std()+1e-6)  # area z-norm\n",
    "    x[:,3:6] = x[:,3:6] / (np.linalg.norm(x[:,3:6], axis=1, keepdims=True)+1e-6)  # centroid direction-ish\n",
    "    return x\n",
    "\n",
    "def load_sample(root: Path, idx: int):\n",
    "    root = Path(root)\n",
    "    pkl_path  = root / f\"{idx}.pkl\"\n",
    "    json_path = root / \"labels\" / f\"{idx}.json\"\n",
    "    if not (pkl_path.exists() and json_path.exists()):\n",
    "        raise FileNotFoundError(idx, pkl_path, json_path)\n",
    "\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        G = pickle.load(f)\n",
    "    with open(json_path, \"r\") as f:\n",
    "        J = json.load(f)\n",
    "\n",
    "    num_nodes = int(G['num_nodes'])\n",
    "    y = np.array(J['per_face_labels'], dtype=np.int64)\n",
    "    if len(y) != num_nodes:\n",
    "        raise ValueError(f\"label/face mismatch for {idx}: {len(y)} vs {num_nodes}\")\n",
    "\n",
    "    x = build_node_features(G['face_features'])              # [n, D]\n",
    "    edge_index = np.asarray(G['edge_index'], dtype=np.int64) # [2, E]\n",
    "\n",
    "    return {\n",
    "        \"x\": torch.from_numpy(x),\n",
    "        \"edge_index\": torch.from_numpy(edge_index),\n",
    "        \"y\": torch.from_numpy(y),\n",
    "        \"idx\": idx\n",
    "    }\n",
    "\n",
    "# ========= PyG dataset with lazy cache =========\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root: Path, ids, cache_mode=\"lazy\", strict=True):\n",
    "        from tqdm import tqdm\n",
    "        self.root = Path(root)\n",
    "        self.ids = []\n",
    "        self._cache = {} if cache_mode in (\"lazy\", \"all\") else None\n",
    "        self._lazy = (cache_mode == \"lazy\")\n",
    "\n",
    "        for i in tqdm(ids, desc=\"Indexing graphs\"):\n",
    "            try:\n",
    "                d = load_sample(self.root, i)\n",
    "                if strict and (d[\"y\"].numel() != d[\"x\"].shape[0]):\n",
    "                    continue\n",
    "                self.ids.append(i)\n",
    "                if cache_mode == \"all\" or (self._cache is not None and not self._lazy):\n",
    "                    self._cache[i] = self._to_data(d)\n",
    "            except Exception:\n",
    "                continue\n",
    "        if not self.ids:\n",
    "            raise RuntimeError(\"No usable graphs after filtering.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_data(d):\n",
    "        return Data(x=d[\"x\"].float(),\n",
    "                    edge_index=d[\"edge_index\"].long(),\n",
    "                    y=d[\"y\"].long())\n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.ids[idx]\n",
    "        if self._cache is not None and pid in self._cache:\n",
    "            return self._cache[pid]\n",
    "        d = load_sample(self.root, pid)\n",
    "        g = self._to_data(d)\n",
    "        if self._cache is not None and self._lazy:\n",
    "            self._cache[pid] = g\n",
    "        return g\n",
    "\n",
    "# ========= model =========\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=128, out_dim=NUM_CLASSES, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.c1 = GCNConv(in_dim, hidden)\n",
    "        self.c2 = GCNConv(hidden, hidden)\n",
    "        self.lin = nn.Linear(hidden, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.c1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.c2(x, edge_index))\n",
    "        return self.lin(x)\n",
    "\n",
    "# ========= training helpers =========\n",
    "def class_weights_from_dataset(ds):\n",
    "    counts = torch.zeros(NUM_CLASSES, dtype=torch.long)\n",
    "    for g in ds:\n",
    "        counts += torch.bincount(g.y, minlength=NUM_CLASSES)\n",
    "    w = 1.0 / (counts.float() + 1e-6)\n",
    "    w *= (NUM_CLASSES / w.sum())\n",
    "    return w\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_labels = 0.0, 0, 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\"), dtype=torch.float16):\n",
    "            logits = model(batch.x, batch.edge_index)\n",
    "            loss = F.cross_entropy(logits, batch.y)\n",
    "        total_loss += float(loss) * batch.y.numel()\n",
    "        total_correct += (logits.argmax(dim=1) == batch.y).sum().item()\n",
    "        total_labels += batch.y.numel()\n",
    "    return total_loss / max(1, total_labels), total_correct / max(1, total_labels)\n",
    "\n",
    "def train_one_epoch(model, device, loader, opt, scaler, weight):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_labels = 0.0, 0, 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\"), dtype=torch.float16):\n",
    "            logits = model(batch.x, batch.edge_index)\n",
    "            w = weight.to(device=device, dtype=logits.dtype) if weight is not None else None\n",
    "            loss = F.cross_entropy(logits, batch.y, weight=w)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        total_loss += float(loss) * batch.y.numel()\n",
    "        total_correct += (logits.argmax(dim=1) == batch.y).sum().item()\n",
    "        total_labels += batch.y.numel()\n",
    "    return total_loss / max(1, total_labels), total_correct / max(1, total_labels)\n",
    "\n",
    "# ========= inference utility =========\n",
    "@torch.no_grad()\n",
    "def predict_part(model, device, root: Path, part_id: int):\n",
    "    d = load_sample(root, part_id)\n",
    "    x = d[\"x\"].to(device)\n",
    "    ei = d[\"edge_index\"].to(device)\n",
    "    logits = model(x, ei)\n",
    "    yhat = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "    return [feat_names[i] for i in yhat]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "avail = get_available_ids(ROOT, max_id=MAX_ID_SCAN)\n",
    "print(f\"usable parts on disk: {len(avail)}\")\n",
    "if len(avail) < 50:\n",
    "    raise RuntimeError(\"Too few usable samples; generate more.\")\n",
    "\n",
    "ds_tr_ids, ds_va_ids, ds_te_ids = split_ids(avail, seed=SEED)\n",
    "print(f\"split -> train {len(ds_tr_ids)}, val {len(ds_va_ids)}, test {len(ds_te_ids)}\")\n",
    "\n",
    "# build datasets (lazy cache), avoid Windows/Jupyter multiprocessing stalls\n",
    "IS_WINDOWS = (sys.platform == \"win32\")\n",
    "IN_NOTEBOOK = (\"ipykernel\" in sys.modules)\n",
    "NUM_WORKERS = 0 if (IS_WINDOWS or IN_NOTEBOOK) else 4\n",
    "pin = torch.cuda.is_available() and NUM_WORKERS > 0\n",
    "\n",
    "ds_tr = GraphDataset(ROOT, ds_tr_ids, cache_mode=\"lazy\", strict=True)\n",
    "ds_va = GraphDataset(ROOT, ds_va_ids, cache_mode=\"lazy\", strict=True)\n",
    "ds_te = GraphDataset(ROOT, ds_te_ids, cache_mode=\"none\", strict=True)\n",
    "\n",
    "tr_loader = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "va_loader = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "te_loader = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "\n",
    "# infer feature dimension from one item\n",
    "g0 = ds_tr[0]\n",
    "in_dim = g0.x.shape[1]\n",
    "print(f\"in_dim: {in_dim}\")\n",
    "\n",
    "model = GCN(in_dim, hidden=HIDDEN, out_dim=NUM_CLASSES, dropout=0.2).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = GradScaler(enabled=(device.type==\"cuda\"))\n",
    "W = class_weights_from_dataset(ds_tr)\n",
    "\n",
    "# train\n",
    "hist = {\"tr_loss\": [], \"va_loss\": [], \"tr_acc\": [], \"va_acc\": []}\n",
    "best_val, best_state, bad = float(\"inf\"), None, 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, device, tr_loader, opt, scaler, W)\n",
    "    va_loss, va_acc = evaluate(model, device, va_loader)\n",
    "\n",
    "    hist[\"tr_loss\"].append(tr_loss); hist[\"va_loss\"].append(va_loss)\n",
    "    hist[\"tr_acc\"].append(tr_acc);   hist[\"va_acc\"].append(va_acc)\n",
    "\n",
    "    print(f\"ep{epoch:03d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "\n",
    "    if va_loss < best_val - 1e-4:\n",
    "        best_val = va_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# save checkpoint\n",
    "ckpt_dir = Path(\"./checkpoints\"); ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_path = ckpt_dir / \"gcn_facecls.pt\"\n",
    "torch.save({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"in_dim\": in_dim,\n",
    "    \"hidden\": HIDDEN,\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "    \"feat_names\": feat_names,\n",
    "    \"train_ids\": ds_tr.ids, \"val_ids\": ds_va.ids, \"test_ids\": ds_te.ids,\n",
    "}, ckpt_path)\n",
    "print(f\"Saved checkpoint -> {ckpt_path}\")\n",
    "\n",
    "# test\n",
    "te_loss, te_acc = evaluate(model, device, te_loader)\n",
    "print(f\"TEST  loss/acc: {te_loss:.4f}/{te_acc:.3f}\")\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(hist[\"tr_loss\"], label=\"train\")\n",
    "plt.plot(hist[\"va_loss\"], label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\")\n",
    "plt.title(\"GCN per-face classification\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# quick demo inference\n",
    "demo_id = random.choice(ds_te.ids)\n",
    "pred_names = predict_part(model, device, ROOT, demo_id)\n",
    "print(f\"\\nDemo predictions for part {demo_id}:\")\n",
    "print(pred_names[:min(20, len(pred_names))], f\"... (total faces={len(pred_names)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick demo inference\n",
    "demo_id = random.choice(ds_te.ids)\n",
    "pred_names = predict_part(model, device, ROOT, demo_id)\n",
    "print(f\"\\nDemo predictions for part {demo_id}:\")\n",
    "print(pred_names[:min(20, len(pred_names))], f\"... (total faces={len(pred_names)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- quick demo inference + flat GT comparison (no functions) ---\n",
    "demo_id = random.choice(ds_te.ids)\n",
    "print(f\"\\n--- Demo part {demo_id} ---\")\n",
    "\n",
    "# load sample + move to device\n",
    "d  = load_sample(ROOT, demo_id)          # also reads ground-truth JSON\n",
    "x  = d[\"x\"].to(device)\n",
    "ei = d[\"edge_index\"].to(device)\n",
    "y  = d[\"y\"].cpu().numpy()                # ground-truth indices\n",
    "\n",
    "# forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\"), dtype=torch.float16):\n",
    "        logits = model(x, ei)            # [num_faces, NUM_CLASSES]\n",
    "\n",
    "# predictions and names\n",
    "pred_idx   = logits.argmax(dim=1).cpu().numpy()\n",
    "pred_names = [feat_names[i] for i in pred_idx]\n",
    "gt_names   = [feat_names[i] for i in y]\n",
    "\n",
    "# accuracy\n",
    "correct = int((pred_idx == y).sum())\n",
    "total   = int(y.size)\n",
    "acc     = correct / max(1, total)\n",
    "print(f\"faces={total}  acc={acc:.3f}  ({correct}/{total})\")\n",
    "\n",
    "# show a few mismatches with top-3 alternatives\n",
    "wrong = np.flatnonzero(pred_idx != y)\n",
    "max_show = 20\n",
    "if wrong.size:\n",
    "    probs = logits.softmax(dim=1).cpu().numpy()\n",
    "    k = min(max_show, wrong.size)\n",
    "    print(f\"First {k} mismatches (face_idx: pred (p) -> gt | top3):\")\n",
    "    for i in wrong[:k]:\n",
    "        top3 = probs[i].argsort()[-3:][::-1]\n",
    "        top3_str = \", \".join(f\"{feat_names[t]}({probs[i][t]:.2f})\" for t in top3)\n",
    "        print(f\"  {i:4d}: {feat_names[pred_idx[i]]} ({probs[i][pred_idx[i]]:.2f})\"\n",
    "              f\" -> {feat_names[y[i]]} | {top3_str}\")\n",
    "else:\n",
    "    print(\"No mismatches on this part ðŸŽ‰\")\n",
    "\n",
    "# optional: per-part classification report (requires scikit-learn)\n",
    "try:\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"\\nClassification report (this part only):\")\n",
    "    print(classification_report(\n",
    "        y, pred_idx,\n",
    "        labels=list(range(NUM_CLASSES)),\n",
    "        target_names=feat_names,\n",
    "        digits=3,\n",
    "        zero_division=0\n",
    "    ))\n",
    "except Exception as e:\n",
    "    print(f\"[sklearn report skipped] {e}\")\n",
    "\n",
    "# optional: peek a few names\n",
    "print(\"\\nPred (first 20):\", pred_names[:20])\n",
    "print(\"GT   (first 20):\", gt_names[:20])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
